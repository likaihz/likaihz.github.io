---
title: CS224N笔记-语言模型与循环神经网络
tags:
  - 笔记
  - NLP
mathjax: true
abbrlink: 3906993302
date: 2019-12-06 15:15:46
---



&#8195;&#8195;斯坦福深度自然语言处理课程CS224N Lecture 6课程笔记，Language Models and Recurrent Neural Networks.

## 语言模型，Language Model

&#8195;&#8195;**语言模型**是用来预测“下一个词”的模型。即给模型输入一个词语序列，输出应该是下一个词（的概率分布）。对这个问题进行形式化的描述如下：

> 给定一个词语序列$$x^{(1)}, x^{(2)},...,x^{(t)}$$，计算下一个词$$x^{(t+1)}$$的概率分布：
> $$
> P(x^{(t+1)}|x^{(t)},...,x^{(1)}) \tag{1}
> $$
> 其中$$x^{(t+1)}$$可以是词汇表 $$V$$ 中的任意一个词。

&#8195;&#8195;也可以从另一个角度看待语言模型，将其看作是计算“一条文本”出现的概率。（一条文本指的就是一串词语）。即对于一条文本：$$x^{(1)},...,x^{(T)}$$ ，其出现的概率是：
$$
\begin{align}
P(x^{(1)},...x^{(T)})&=P(x^{(1)})\times P(x^{(2)}|x^{(1)})\times ... \times P(x^{(T)}|x^{(T-1)},...,x^{(1)}) \tag{2} \\
&=\prod_{t=1}^{T}P(x^{(t)}|x^{(t-1)},...,x^{(1)}) \tag{3}
\end{align}
$$
$$(3)$$中的概率就是由语言模型给出的。



<!--more-->



## n-gram语言模型

### 模型

* 假设：$$x^{(t+1)}$$的概率分布仅与其前$$n-1$$个词有关
* 那么就可以得到其概率分布为：

$$
\begin{align}
P(x^{(t+1)}|x^{(t)},...,x^{(1)})&=P(x^{(t+1)}|x^{(t)},...,x^{(t-n+2)}) \tag{4} \\
&=\frac{P(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{P(x^{(t)},...,x^{(t-n+2)})}\tag{5} \\
&\approx \frac{count(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{count(x^{(t)},...,x^{(t-n+2)})}\tag{6}
\end{align}
$$

### 缺陷

* 模型缺陷1：稀疏性问题
  * 公式$$(6)$$中，如果某个词序列在语料库中从来没有出现过，那么分子可能为0。为了解决这个问题，可以在每个计数值上加一个很小的平滑量（smoothing value）。
  * 分母也可能为0（即在语料库中未出现过这样的上文），为了解决该问题，可以缩小上文的长度。

* 模型缺陷2：容量问题
  * 模型需要计算并保存在语料库中出现的所有n-gram，那么随着n的增长，模型的大小也快速增长。



## 固定窗口大小的神经网络语言模型

* 基本假设与n-gram模型类似，预测某个位置的词的概率分布只与前n个词有关。
* 构建一个基本的多层感知机(MLP)，输入前n个词，输出为下一个词。
* 仍存在问题。。。



## 循环神经网络，Recurrent Neural Network

### 模型

* 基本做法：重复地调用相同的权重矩阵。
* RNN的基本结构及展开结构如下图

{% asset_img rnn.png rnn %}

可以看到，其基本结构就类似一个一维的多层感知机，每层有各自的权重矩阵。区别在于，隐藏层有两个输入来源——输入层的激活输出和上一个时间步的隐藏层激活输出，（$$W_e$$和$$W_h$$是各自的权重矩阵。）



### Pros and cons

* 优点
  * 可以处理任意长度的输入
  * 理论上来说，计算每一步的输出时都是基于前面所有的信息
  * 模型的大小不会随着输入增长而变大
  * 因为使用的是同一个权重矩阵，因此处理输入时是类似的
* 缺点
  * 循环计算非常慢
  * 实践中，很难使用到很多步之前的信息。



## 评价语言模型

* 标准的语言模型评价指标——perplextiy，该指标越小，模型表现越好。

$$
\prod_{t=1}^{T}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},...,x^{(1)})})^{1/T}
$$



## RNN的应用

### 打标签

* 如词性分类、命名体识别等

{% asset_img tagging.png tagging %}



### 句子（文本）分类

* 如情感分类等
* 在RNN的输出层后再加一层，用于将RNN的输出做编码



{% asset_img sentence.png sentence %}



### 作为编码模块

* 利用RNN为一条文本编码，用编码后的结果参与后续神经网络的计算