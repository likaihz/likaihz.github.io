---
title: Word2Vec教程： Skip-Gram模型
tags:
- 翻译
- NLP
- word2vec
abbrlink: 863537285
date: 2019-11-05 20:16:50
---

最近在看CS224N，本文是第一周的阅读任务之一([原文链接](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/))，以下是全文翻译。

&#8195;&#8195;该教程讲解了Word2Vec中的Skip-Gram神经网络结构，教程的目标是深入讲解细节。

## 模型

&#8195;&#8195;Skip-Gram神经网络模型的基本形式非常简单，如果从细节上的调整和优化开始解释则会显得有些混乱。

&#8195;&#8195;我们先从一个整体的视角去考虑，Word2Vec使用了机器学习中比较常见的的一个技巧，一般来说我们训练一个具有单个隐藏层的简单神经网络来执行特定任务，但是在训练出模型后并不会使用该神经网络来执行该任务，我们要的只是神经网络中隐藏层的权重——这些权重实际上就是我们想要得到的“词向量”。

&#8195;&#8195;在无监督的特征学习中我们也见过该技巧，即训练自动编码器在隐藏层中压缩输入向量，然后将其解压缩回输出层中的原始向量。训练它之后，可以剥离输出层（解压步骤），而仅使用隐藏层-这是学习良好图像特征而无需标记训练数据的技巧。

<!--more-->

## 虚设的任务

&#8195;&#8195;那么我们现在需要讨论这个“虚设”的任务，即以该任务为目标构建神经网络并在完成后再回过头来看看这个神经网络如何间接地给出我们真正想要的词向量。

&#8195;&#8195;我们将训练神经网络达到以下目的（译者注：即定义该虚设任务）：给定句子中的一个词（输入词），查看附近的词语并随机选择一个。神经网络将告诉我们词汇集中每个词成为我们选择的“附近单词”的可能性。

> 当我提到“附近的词”时，实际上指的是算法中的一个名为“窗口大小”的参数。一般来说窗口大小设为5，即中心词之前的5个词和之后的5个词（一共10个词）

&#8195;&#8195;输出概率将与找到输入单词附近的每个词汇单词的可能性有关。 例如，如果给已训练的网络输入单词“Soviet”，那么“Union”和“Russia”这样的单词的输出概率就会比“watermelon”和“kangaroo”这样的无关单词的输出概率高得多。

&#8195;&#8195;我们将通过向神经网络提供训练文档中找到的单词对来训练神经网络。 下面的示例显示了一些从句子“The quick brown fox jumps over the lazy dog.”中获取的训练示例（单词对），我仅以2小窗口为例。以蓝色突出显示的单词是输入单词。

{% asset_img training-samples.png training-samples %}

&#8195;&#8195;网络将从每个配对出现的次数中学习统计信息。那么举例来说，相比（“Soviet”，“Sasquatch”）网络可能会获得更多的（“Soviet”，“Union”）训练样本。训练结束后，如果输入“Soviet”一词，则“Union”或“Russia”的概率要比“Sasquatch”的概率高得多。

## 模型细节

&#8195;&#8195;那么这一切是如何表示的呢？

&#8195;&#8195;首先，我们不能以字符串形式向神经网络输入单词，因此我们需要一种将单词表示给网络的方法。为此，我们首先从训练文档中构建词汇集——假设我们有大小为10000个词汇集。

&#8195;&#8195;我们将把“ants”之类的输入词表示为一个one-hot向量。此向量将包含10,000个分量（每个分量对应词汇中的一个词），我们将在与“ants”对应的位置置“ 1”，在所有其他位置置0。

&#8195;&#8195;网络的输出是单个向量（也具有10,000个分量），表示的是词汇表中的其他每一个词是上下文词的概率。

&#8195;&#8195;以下是我们的神经网络的结构

{% asset_img arch.png arch %}

&#8195;&#8195;隐藏层神经元上没有激活函数，但是输出神经元使用softmax。我们稍后会再讨论。

&#8195;&#8195;当用单词对训练此网络时，输入是代表输入单词的one-hot向量，训练输出也是代表输出单词的one-hot向量。但是，当使用经过训练的网络预测时，输出向量实际上将是概率分布（即一堆浮点值，而不是one-hot向量）。

## 隐藏层

&#8195;&#8195;对于我们的示例，我们要学习的词向量有300维。因此，隐藏层将由权重矩阵表示，该矩阵具有10,000行（每一行对应词汇表中的一个单词）和300列（每个隐藏神经元一个）。

> Google在Google新闻数据集（可以从[此处](https://code.google.com/archive/p/word2vec/)下载）上训练的已发布模型中使用了300个特征。特征的数量是需要根据实际应用调整的“超参数”（即，尝试不同的值并查看产生最佳结果的参数）。

&#8195;&#8195;如果再看一下此权重矩阵的行，这些实际上就是我们的单词向量。

{% asset_img matrix.png matrix %}

&#8195;&#8195;因此，所有这一切的最终目标实际上只是学习此隐藏层权重矩阵——训练完成后，我们将丢弃输出层。

&#8195;&#8195;现在我们再回到模型的定义中。

&#8195;&#8195;你可能会问自己：“那个one-hot向量几乎全为零……这有什么作用？”如果您将一个1x10,000的one-hot向量乘以10,000x300矩阵，它的结果实际上就是在one-hot向量中为“1”那一行所对应的矩阵行，下面的例子可以更直观地看出。

{% asset_img multiply.png multiply %}

&#8195;&#8195;这意味着该模型的隐藏层实际上只是用作查找表，隐藏层的输出只是输入单词的“单词向量”。

## 输出层

&#8195;&#8195;隐藏层将表示“ants”的1x300词向量送入输出层，输出层是softmax回归分类器，[这里](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)有一个关于Softmax回归的深入教程，教程的要点是每个输出神经元（每个神经元代表词汇集中的一个单词）将产生0到1之间的输出，所有这些输出值的总和为1。

&#8195;&#8195;具体来说，每个输出神经元都有一个权重向量，该向量与隐藏层的单词向量相乘，然后将函数exp（x）应用于结果。最后，为了使输出总和为1，我们将这个结果除以所有10,000个输出节点的结果之和。

&#8195;&#8195;下面是计算单词“ car”的输出神经元输出的示例。

{% asset_img carweights.png carweights %}

> 注意，神经网络对输出词相对于输入词在语料中位置的距离一无所知。对于输入单词前后的单词，它不会学习不同的概率集。举例来说，如果在我们的训练语料库中，每一次出现单词“York”，其前面一个单词总是“New”，即根据训练数据，“New”将在“ York”附近的可能性为100％。但是，如果我们在“York”附近选择10个单词(译者注：即前文提到的窗口大小为5)，并随机选择其中一个，那么选到“New”的可能性并不是100％因为可能在附近选择了其他单词。

## 直觉推断

&#8195;&#8195;下面你准备好对这个网络进行一些更有意思的探索了吗？

&#8195;&#8195;如果两个不同的词具有非常相似的“上下文”（上下文指它们周围可能出现的词），那么我们的模型需要为这两个词输出非常接近的结果。直觉来说，如果两个词的的词向量相似，则网络为这两个单词输出上下文的概率分布也是比较相似的。因此，如果两个单词具有相似的上下文，则我们的网络将为这两个单词学习相似的单词向量。

&#8195;&#8195;两个单词具有相似的上下文是什么意思？我想你会同意“智能”和“智能”等同义词具有非常相似的上下文。或者，相关的词（例如“引擎”和“变速器”）也可能具有相似的上下文。

&#8195;&#8195;这也可以为您处理词干（stemming）——网络可能会为“ant”和“ants”学习相似的词向量，因为它们应该具有相似的上下文。

## 后续

&#8195;&#8195;你可能已经注意到，skip-gram神经网络包含大量权重。我们的示例中有300个特征和10,000个单词，隐藏层和输出层中的权重均为3,000,000，在如此大型的数据集上进行训练的代价很大，因此word2vec的作者进行了许多调整以使训练可行，这些将在本教程的[第2部分](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)中介绍。

## 其他资料

&#8195;&#8195;您是否知道word2vec模型也可以应用于非文本数据，以用于推荐系统和广告定位？ 您可以从一系列用户操作中学习向量，而不是从一系列单词中学习向量。在我的[新文章](http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/)中阅读有关此内容的更多信息。
