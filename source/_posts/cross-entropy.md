---
title: 从信息熵到交叉熵
mathjax: true
abbrlink: 2352984677
date: 2019-11-15 14:57:22
tags:
- 机器学习
---

## 信息熵，Information entropy

熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以信息熵也被称为香农熵 (Shannon entropy)，用来度量信息量的大小。

### 信息量

&#160; &#160; &#160; &#160;信息(Information)是一个很抽象的概念，其定义不统一，这里简单认为信息描述了一个或多个事件。那么不严谨地说，一条信息描述的事件越多么，其信息量越大；再进一步说，当信息描述的事件发生的可能性越小时，其信息量越大。这个原理比较复杂，不过可以从直觉上进行简单的解释，因为在一个事件系统中，当一个可能性很小的事件发生时，其一般都产生于多个前提条件，因此描述了该事件的信息也可以认为携带了其前提条件的信息，因此信息量较大。举例来说，在当前时空下，有两条信息分别描述了事件A：“中国队获得2022年世界杯冠军。”和事件B：“德国队获得2022你啊世界杯冠军。”，显然（？）事件A的可能性很小，那么这条信息就包含了中国队经过换教练、换主力队员，国内足球运动改革等等举措以及天大的运气最终获得冠军。而描述事件B的信息就简单的多，常规操作就可以。

&#160; &#160; &#160; &#160;所以从直觉上来说，当信息描述的事件发生的可能性越小时，其信息量就越大（当然上面的推理过程并不严谨）；那么信息量应该和事件发生的概率有关。假设$X$是一个离散型随机变量，$p(x)$是其概率分布函数，那么可以定义事件$X = x_0$的信息量为：
$$
I(x_0) = -\log(p(x_0))
$$

### 信息熵

&#160; &#160; &#160; &#160;有了信息量的概念后，就可以定义信息熵了。根据Boltzmann's H-theorem，香农把随机变量X的信息量的期望定义为熵值H，记X的值域为$\chi=\{x_1, x_2, ..., x_n\}$，那么有：
$$
H(X) = - \sum_{i=1}^{n}p(x_i)\log(p(x_i))
$$
$H(x)$就被称为随机变量$X$的熵，表示随机变量不确定性的度量，是对所有可能发生的事件产生的信息量的期望。
从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大，且$0 \le H(X) \le \log n$。并且由于信息熵仅于变量分布有关，因此也可以记为$H(P)$

## 相对熵（KL散度）

&#160; &#160; &#160; &#160;相对熵（relative entropy）又称为KL散度（Kullback-Leibler divergence, KLD），它是两个概率分布P和Q的差别的非对称性的度量。从信息角度来说，KLD(P||Q)表示用P描述分布时，相对用Q描述分布的信息增量。在机器学习中，P表示样本的真实分布，Q表示模型所预测的分布，那么如果用P来描述样本分布，就不存在信息上的差异（因为P就是样本的实际分布）；但是如果用Q来描述，虽然已经近似，但是信息量仍然不足，需要一些额外的“信息增量”才能完美描述分布。训练过程实际就是缩小这一点“信息增量”的过程。

&#160; &#160; &#160; &#160;KL散度的计算公式：

$$
D_{KL}(P||Q)=\sum_{i=1}^{n}p(x_i)\log(\frac{p(x_i)}{q(x_i)})
$$
KL散度的值越小，P和Q两种分布就越接近，在机器学习中，也就是训练出的Q模型越接近实际分布P。

## 交叉熵

&#160; &#160; &#160; &#160;从相对熵的定义中可以看出，相对熵值越小，表示P和Q两种分布越接近，那么相对熵就可以作为机器学习的训练目标——即训练过程以最小化该值为目标。但是最小化相对熵还不是最简形式，对相对熵的计算公式简单变形：
$$
\begin{split}
D_{KL}(P||Q) &= \sum_{i=1}^{n}p(x_i)\log(p(x_i))-\sum_{i=1}^{n}p(x_i)\log(q(x_i))\\
 &= -H(P)+[-\sum_{i=1}^{n}p(x_i)\log(q(x_i))]
\end{split}
$$
可以看到，等式的前半部分是P分布的信息熵，其不会改变，所以在训练的过程中，只要取等式的后半部分作为目标函数。对这一部分定义为交叉熵：

$$
H(P,Q)=-\sum_{i=1}^{n}p(x_i)\log(q(x_i))
$$
